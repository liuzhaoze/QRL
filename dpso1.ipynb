{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import random\n",
    "import optuna\n",
    "import ray\n",
    "from collections import deque\n",
    "from ray import tune\n",
    "from ray.tune.integration.wandb import WandbLoggerCallback\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from typing import List, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "from mlflow import log_metric, log_param, start_run, end_run\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "import ipyvolume as ipv\n",
    "import ipywidgets as widgets\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "class Config(BaseModel):\n",
    "    task_count: int = 8000\n",
    "    vm_count: int = 10\n",
    "    task_arrival_rate: float = 40.0\n",
    "    task_compute_mean: int = 200\n",
    "    task_compute_std: int = 20\n",
    "    vm_capacity: List[int] = Field(default_factory=lambda: [1000, 1000, 1100, 1100, 1100, 1100, 1200, 1200, 1200, 1200])\n",
    "    vm_attribute: List[int] = Field(default_factory=lambda: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
    "\n",
    "@hydra.main(config_path=\".\", config_name=\"config\")\n",
    "def main(cfg: DictConfig):\n",
    "\n",
    "    task_arrival_times = np.cumsum(np.random.exponential(1.0 / cfg.task_arrival_rate, cfg.task_count))\n",
    "\n",
    "    try:\n",
    "        with Dataset('task_data.nc', 'w', format='NETCDF4') as ncfile:\n",
    "            arrival_var = ncfile.createVariable('task_arrival_times', np.float32, ('tasks',))\n",
    "            arrival_var[:] = task_arrival_times\n",
    "    except Exception as e:\n",
    "        print(f\"Error handling netCDF4 file: {str(e)}\")\n",
    "\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "\n",
    "    class VirtualMachine:\n",
    "        def __init__(self, capacity: int, attribute: int):\n",
    "            self.capacity = capacity\n",
    "            self.attribute = attribute\n",
    "            self.queue = deque()\n",
    "            self.available_time = 0\n",
    "\n",
    "        def add_task(self, task: Tuple[float, int, int], current_time: float) -> Tuple[float, float]:\n",
    "            start_time = max(self.available_time, current_time)\n",
    "            exec_time = (task[1] / self.capacity) * (2 if task[2] != self.attribute else 1)\n",
    "            self.available_time = start_time + exec_time\n",
    "            self.queue.append((task, start_time, self.available_time))\n",
    "            return start_time, self.available_time\n",
    "\n",
    "    try:\n",
    "        with Dataset('vm_data.nc', 'w', format='NETCDF4') as ncfile:\n",
    "            ncfile.createDimension('vms', cfg.vm_count)\n",
    "            vm_capacity_var = ncfile.createVariable('vm_capacity', np.int32, ('vms',))\n",
    "            vm_attribute_var = ncfile.createVariable('vm_attribute', np.int32, ('vms',))\n",
    "            vm_capacity_var[:] = cfg.vm_capacity\n",
    "            vm_attribute_var[:] = cfg.vm_attribute\n",
    "    except Exception as e:\n",
    "        print(f\"Error handling netCDF4 file: {str(e)}\")\n",
    "\n",
    "    vms = [VirtualMachine(cfg.vm_capacity[i], cfg.vm_attribute[i]) for i in range(cfg.vm_count)]\n",
    "\n",
    "    class Scheduler:\n",
    "        def __init__(self, vms: List[VirtualMachine]):\n",
    "            self.vms = vms\n",
    "            self.iteration_scores = []\n",
    "            self.global_best_scores = []\n",
    "            self.swarm_positions = []\n",
    "\n",
    "        def reset(self):\n",
    "            for vm in self.vms:\n",
    "                vm.queue.clear()\n",
    "                vm.available_time = 0\n",
    "\n",
    "        def dpso_scheduler(self, tasks, iterations=100, swarm_size=20, w=0.5, c1=2.5, c2=2.5) -> float:\n",
    "            swarm = [np.random.randint(0, len(self.vms), size=len(tasks)) for _ in range(swarm_size)]\n",
    "            personal_best = swarm.copy()\n",
    "            personal_best_scores = [self.evaluate_schedule(tasks, particle) for particle in swarm]\n",
    "            global_best = personal_best[np.argmin(personal_best_scores)]\n",
    "            global_best_score = min(personal_best_scores)\n",
    "            velocities = [np.random.randint(-1, 2, size=len(tasks)) for _ in range(swarm_size)]\n",
    "\n",
    "            fig = ipv.figure()\n",
    "            scatter = ipv.scatter([], [], [], marker=\"sphere\", size=1)\n",
    "\n",
    "            for iteration in range(iterations):\n",
    "                iteration_best_score = float('inf')\n",
    "                positions = []\n",
    "\n",
    "                for i in range(swarm_size):\n",
    "                    r1, r2 = random.random(), random.random()\n",
    "                    inertia = w * velocities[i]\n",
    "                    cognitive = c1 * r1 * (personal_best[i] - swarm[i])\n",
    "                    social = c2 * r2 * (global_best - swarm[i])\n",
    "                    velocities[i] = np.clip(inertia + cognitive + social, -1, 1)\n",
    "\n",
    "                    swarm[i] = (swarm[i] + velocities[i]) % len(self.vms)\n",
    "                    score = self.evaluate_schedule(tasks, swarm[i])\n",
    "                    iteration_best_score = min(iteration_best_score, score)\n",
    "\n",
    "                    positions.append(swarm[i].copy())\n",
    "                    if score < personal_best_scores[i]:\n",
    "                        personal_best[i] = swarm[i].copy()\n",
    "                        personal_best_scores[i] = score\n",
    "\n",
    "                    if score < global_best_score:\n",
    "                        global_best = swarm[i].copy()\n",
    "                        global_best_score = score\n",
    "\n",
    "                self.iteration_scores.append(iteration_best_score)\n",
    "                try:\n",
    "                    with Dataset('dpso_scores.nc', 'a') as ncfile:\n",
    "                        if 'iteration_scores' not in ncfile.variables:\n",
    "                            score_var = ncfile.createVariable('iteration_scores', np.float32, ('iterations',))\n",
    "                        score_var[iteration] = iteration_best_score\n",
    "                except Exception as e:\n",
    "                    print(f\"Error writing to netCDF4 file: {str(e)}\")\n",
    "\n",
    "                scatter.x = [np.mean(pos) for pos in positions]\n",
    "                scatter.y = [np.std(pos) for pos in positions]\n",
    "                scatter.z = [global_best_score] * len(positions)\n",
    "                ipv.clear()\n",
    "                ipv.show()\n",
    "\n",
    "            self.reset()\n",
    "            total_time = 0\n",
    "            for task_idx, (task, vm_index) in enumerate(zip(tasks, global_best)):\n",
    "                vm = self.vms[vm_index]\n",
    "                start_time, end_time = vm.add_task((task.arrival_time, task.compute_req, task.attribute), task.arrival_time)\n",
    "                total_time += (end_time - task.arrival_time)\n",
    "            return total_time / len(tasks)\n",
    "\n",
    "        def evaluate_schedule(self, tasks, schedule) -> float:\n",
    "            self.reset()\n",
    "            total_time = 0\n",
    "            for task, vm_index in zip(tasks, schedule):\n",
    "                vm = self.vms[vm_index]\n",
    "                start_time, end_time = vm.add_task((task.arrival_time, task.compute_req, task.attribute), task.arrival_time)\n",
    "                total_time += (end_time - task.arrival_time)\n",
    "            return total_time / len(tasks)\n",
    "\n",
    "    scheduler = Scheduler(vms)\n",
    "\n",
    "    wandb.init(project=\"dpso-advanced\", name=\"DPSO with PBT and MLflow\")\n",
    "    start_run()\n",
    "\n",
    "    def objective(trial):\n",
    "        iterations = trial.suggest_int('iterations', 100, 500)\n",
    "        swarm_size = trial.suggest_int('swarm_size', 100, 200)\n",
    "        w = trial.suggest_float('w', 0.1, 1.5)\n",
    "        c1 = trial.suggest_float('c1', 0.5, 5.0)\n",
    "        c2 = trial.suggest_float('c2', 0.5, 5.0)\n",
    "        wandb.log({\"iterations\": iterations, \"swarm_size\": swarm_size, \"w\": w, \"c1\": c1, \"c2\": c2})\n",
    "\n",
    "        avg_completion_time = scheduler.dpso_scheduler(tasks, iterations=iterations, swarm_size=swarm_size, w=w, c1=c1, c2=c2)\n",
    "        log_metric(\"avg_completion_time\", avg_completion_time)\n",
    "        wandb.log({\"avg_completion_time\": avg_completion_time})\n",
    "        return avg_completion_time\n",
    "\n",
    "    pbt = PopulationBasedTraining(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"avg_completion_time\",\n",
    "        mode=\"min\",\n",
    "        perturbation_interval=5,\n",
    "        hyperparam_mutations={\n",
    "            \"w\": tune.uniform(0.1, 1.5),\n",
    "            \"c1\": tune.uniform(0.5, 5.0),\n",
    "            \"c2\": tune.uniform(0.5, 5.0)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    analysis = tune.run(\n",
    "        objective,\n",
    "        name=\"dpso_pbt_optuna\",\n",
    "        scheduler=pbt,\n",
    "        num_samples=50,\n",
    "        callbacks=[WandbLoggerCallback(project=\"advanced-dpso-pbt\")],\n",
    "    )\n",
    "\n",
    "    end_run()\n",
    "    ray.shutdown()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
