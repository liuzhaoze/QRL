{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from env import SchedulingEnv\n",
    "from model import baseline_DQN, baselines\n",
    "from utils import get_args\n",
    "from args_config import args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 记录结果用于评估\n",
    "performance_lamda = np.zeros(args.Baseline_num)\n",
    "performance_success = np.zeros(args.Baseline_num)\n",
    "performance_util = np.zeros(args.Baseline_num)\n",
    "performance_finishT = np.zeros(args.Baseline_num)\n",
    "performance_cost = np.zeros(args.Baseline_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 启动环境\n",
    "env = SchedulingEnv(args)\n",
    "\n",
    "# 建立DQN agent\n",
    "brainRL = baseline_DQN(env.actionNum, env.s_features)\n",
    "\n",
    "# 建立其他方法\n",
    "brainOthers = baselines(env.actionNum, env.VMtypes)\n",
    "\"\"\"\n",
    "    parm:\n",
    "        env.actionNum:动作空间数量\n",
    "        env.VMtypes:VM 的类型\n",
    "\"\"\"\n",
    "global_step = 0\n",
    "my_learn_step = 0\n",
    "DQN_Reward_list = []\n",
    "for episode in range(args.Epoch):   #  5轮\n",
    "    print('----------------------------Episode', episode, '----------------------------')\n",
    "    job_c = 1\n",
    "    performance_c = 0\n",
    "    env.reset(args)\n",
    "    performance_respTs = []\n",
    "    while True:\n",
    "        # baseline DQN\n",
    "        global_step += 1\n",
    "\n",
    "        #   获得任务属性\n",
    "        finish, job_attrs = env.workload(job_c)\n",
    "\n",
    "        #   获得虚拟机和环境状态\n",
    "        DQN_state = env.getState(job_attrs, 4)\n",
    "        # print(DQN_state)\n",
    "        \n",
    "        #   选择虚拟机执行任务，获得reward并记录状态迁移（s,a,s',r）;当已经调度的任务个数满足要求，开始学习\n",
    "        if global_step != 1:\n",
    "            brainRL.store_transition(last_state, last_action, last_reward, DQN_state)\n",
    "\n",
    "        action_DQN = brainRL.choose_action(DQN_state)  # choose action\n",
    "        reward_DQN = env.feedback(job_attrs, action_DQN, 4)\n",
    "\n",
    "        if episode == 1:\n",
    "            DQN_Reward_list.append(reward_DQN)\n",
    "\n",
    "        if (global_step > args.Dqn_start_learn) and (global_step % args.Dqn_learn_interval == 0):  # learn\n",
    "            brainRL.learn()\n",
    "            \n",
    "        last_state = DQN_state\n",
    "        last_action = action_DQN\n",
    "        last_reward = reward_DQN\n",
    "        print(brainRL.print_buffer)\n",
    "\n",
    "        # random policy\n",
    "        state_Ran = env.getState(job_attrs, 1)\n",
    "        action_random = brainOthers.random_choose_action()\n",
    "        reward_random = env.feedback(job_attrs, action_random, 1)\n",
    "        # round robin policy\n",
    "        state_RR = env.getState(job_attrs, 2)\n",
    "        action_RR = brainOthers.RR_choose_action(job_c)\n",
    "        reward_RR = env.feedback(job_attrs, action_RR, 2)\n",
    "        # earliest policy\n",
    "        idleTimes = env.get_VM_idleT(3)  # get VM state\n",
    "        action_early = brainOthers.early_choose_action(idleTimes)\n",
    "        reward_early = env.feedback(job_attrs, action_early, 3)\n",
    "\n",
    "        if job_c % 500 == 0:\n",
    "            # 获取 500次 处理的奖励总和\n",
    "            acc_Rewards = env.get_accumulateRewards(args.Baseline_num, performance_c, job_c)\n",
    "            # 获取 500次 处理的奖励总和\n",
    "            cost = env.get_accumulateCost(args.Baseline_num, performance_c, job_c)\n",
    "            # 获取 500次 处理的结束时间总和\n",
    "            finishTs = env.get_FinishTimes(args.Baseline_num, performance_c, job_c)\n",
    "            # 获取 500次 处理的任务执行时间总和\n",
    "            avg_exeTs = env.get_executeTs(args.Baseline_num, performance_c, job_c)\n",
    "            # 获取 500次 处理的等待时间总和\n",
    "            avg_waitTs = env.get_waitTs(args.Baseline_num, performance_c, job_c)\n",
    "            # 获取 500次 处理的响应时间总和\n",
    "            avg_respTs = env.get_responseTs(args.Baseline_num, performance_c, job_c)\n",
    "\n",
    "            performance_respTs.append(avg_respTs)\n",
    "            # 获取 500次 处理的达标率\n",
    "            successTs = env.get_successTimes(args.Baseline_num, performance_c, job_c)\n",
    "            # 设定游标\n",
    "            performance_c = job_c\n",
    "\n",
    "        job_c += 1\n",
    "        if finish:\n",
    "            break\n",
    "\n",
    "    # episode performance\n",
    "    startP = 2000\n",
    "\n",
    "    total_Rewards = env.get_totalRewards(args.Baseline_num, startP)\n",
    "    avg_allRespTs = env.get_total_responseTs(args.Baseline_num, startP)\n",
    "    total_success = env.get_totalSuccess(args.Baseline_num, startP)\n",
    "    avg_util = env.get_avgUtilitizationRate(args.Baseline_num, startP)\n",
    "    total_Ts = env.get_totalTimes(args.Baseline_num, startP)\n",
    "    total_cost = env.get_totalCost(args.Baseline_num, startP)\n",
    "    print('total performance (after 2000 jobs):')\n",
    "    for i in range(len(args.Baselines)):\n",
    "        name = \"[\" + args.Baselines[i] + \"]\"\n",
    "        print(name + \" reward:\", total_Rewards[i], ' avg_responseT:', avg_allRespTs[i],\n",
    "              'success_rate:', total_success[i], ' utilizationRate:', avg_util[i], ' finishT:', total_Ts[i], 'Cost:',\n",
    "              total_cost[i])\n",
    "\n",
    "    if episode != 0:\n",
    "        performance_lamda[:] += env.get_total_responseTs(args.Baseline_num, 0)\n",
    "        performance_success[:] += env.get_totalSuccess(args.Baseline_num, 0)\n",
    "        performance_util[:] += env.get_avgUtilitizationRate(args.Baseline_num, 0)\n",
    "        performance_finishT[:] += env.get_totalTimes(args.Baseline_num, 0)\n",
    "        performance_cost += env.get_totalCost(args.Baseline_num, 0)\n",
    "print('')\n",
    "\n",
    "print('---------------------------- Final results ----------------------------')\n",
    "performance_lamda = np.around(performance_lamda / (args.Epoch - 1), 3)\n",
    "performance_success = np.around(performance_success / (args.Epoch - 1), 3)\n",
    "performance_util = np.around(performance_util / (args.Epoch - 1), 3)\n",
    "performance_finishT = np.around(performance_finishT / (args.Epoch - 1), 3)\n",
    "performance_cost = np.around(performance_cost / (args.Epoch - 1), 3)\n",
    "print('avg_responseT:')\n",
    "print(performance_lamda)\n",
    "print('success_rate:')\n",
    "print(performance_success)\n",
    "print('utilizationRate:')\n",
    "print(performance_util)\n",
    "print('finishT:')\n",
    "print(performance_finishT)\n",
    "print('Cost:')\n",
    "print(performance_cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfq3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
